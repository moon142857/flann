#!/usr/bin/env python
import pyflann
import pickle
import numpy as np
import math 
from sklearn.preprocessing import normalize
 
A1 = [[0.027904,0.008165,-0.016216,0.038300,-0.064714,0.091668,-0.125231,0.016729,0.041737,0.073088,0.050529,0.074768,0.092020,0.021182,0.037864,-0.036610,-0.014878,0.004795,0.023840,-0.014669,-0.053501,0.025635,0.007833,-0.019397,-0.004728,-0.041092,0.016046,0.067942,-0.012732,0.111436,0.105730,-0.036629,0.013017,0.041272,-0.016910,-0.010036,-0.001377,-0.037550,-0.025227,-0.116117,0.010197,-0.082421,0.116458,-0.004367,-0.025777,-0.069945,-0.152072,-0.061343,-0.095305,0.055599,0.022037,-0.001462,-0.145644,0.180299,-0.093235,0.014251,0.083256,0.005355,-0.054308,0.006741,0.008279,0.062321,-0.009001,0.006057,-0.053757,-0.082459,-0.073677,-0.070126,0.025379,-0.005184,0.023727,-0.036022,0.065692,-0.051469,0.090263,-0.106024,0.004320,-0.056368,-0.037607,-0.030002,0.053710,-0.083570,0.064533,-0.097697,0.018505,0.006371,0.080323,-0.019017,-0.134821,0.143204,-0.014868,-0.127064,-0.040816,-0.003247,-0.087301,-0.050425,-0.077484,0.016425,-0.033297,0.102350,-0.078595,0.092428,-0.050168,-0.017232,-0.247538,0.079639,-0.007425,0.038319,0.049409,0.004795,0.089314,0.013662,-0.029746,-0.021590,-0.115404,-0.015713,0.079117,-0.005991,0.052646,-0.205962,0.064533,-0.002791,0.165725,0.041073,-0.021600,-0.055409,-0.093226,0.018932,-0.037949,0.017422,-0.039839,-0.008982,-0.038975,-0.004348,-0.131478,0.014640,-0.057868,0.001709,0.006523,-0.036202,-0.088488,0.119791,0.053197,0.136957,-0.051213,-0.018780,-0.073847,0.109395,-0.137935,0.081035,-0.044918,0.089266,0.021999,-0.199145,0.078129,-0.020100,0.094431,0.043057,-0.095675,0.041234,-0.161899,0.161965,-0.016254,0.104752,-0.008773,0.088421,-0.006190,-0.013634,-0.007709,0.046874,-0.108939,-0.029214,-0.034360,-0.043864,0.046722,-0.047368,0.092580,-0.008260,-0.068806,-0.040987,-0.042174,0.046285,-0.051830,0.157256,0.107306,0.001082,0.031873,-0.016900,0.013957,-0.124680,0.004861,-0.035243]]
A2 = [[-0.102066,0.169254,0.020833,-0.061236,-0.110985,0.078217,-0.129061,-0.107352,0.006430,0.068502,0.102634,0.013288,0.107043,-0.058021,0.000538,-0.050117,0.031255,-0.029364,0.072195,0.017310,0.026378,-0.049431,0.024546,-0.020196,-0.035794,-0.047828,0.106655,0.036053,-0.010312,0.109213,0.120003,0.059563,0.034321,0.009864,0.050635,-0.101320,-0.011517,-0.021819,0.047290,-0.001314,0.061952,-0.046882,0.076186,0.090122,-0.060867,-0.153617,0.026059,-0.066352,-0.021460,-0.003653,-0.004320,0.083254,-0.020973,0.265368,-0.162177,0.028826,-0.003534,0.045648,-0.100394,-0.046723,0.074116,-0.101608,-0.012820,-0.083015,0.002588,-0.049421,0.048863,-0.099558,0.016444,0.048923,-0.066820,0.005047,-0.030707,-0.050914,0.050944,-0.025601,0.074892,0.030110,-0.078874,0.066282,-0.045578,-0.086817,-0.036033,-0.015030,-0.049371,-0.048525,0.068542,-0.029861,-0.109283,0.114260,0.012602,-0.099558,0.088828,0.014642,-0.071538,-0.051919,-0.009914,-0.087175,-0.007067,0.075121,-0.098771,0.022864,0.028796,0.017160,-0.075081,0.063017,0.027910,-0.093775,-0.015538,0.026885,0.084398,0.040821,0.035445,-0.070961,-0.109641,-0.074395,0.100195,-0.005405,0.138189,-0.140050,0.010860,-0.004041,0.116290,0.001394,0.014911,-0.074106,-0.004678,0.018932,-0.083562,0.019161,0.010352,-0.029294,-0.120829,0.065814,-0.080576,0.009357,-0.095148,0.055413,0.054806,0.000707,-0.032360,0.036292,0.057015,0.099697,-0.035764,-0.051073,-0.091864,-0.016205,-0.068850,0.128603,0.010103,-0.001891,-0.020594,-0.130614,-0.021062,-0.076784,0.046634,-0.050973,-0.053462,0.067507,-0.179547,-0.026258,0.004459,0.033853,0.000269,0.145783,-0.019718,-0.050804,0.017399,0.068960,-0.092650,0.001732,0.009924,-0.066004,0.073200,-0.078097,0.094233,-0.041955,-0.013677,-0.063664,-0.027164,0.083881,0.098951,0.111144,0.011686,0.197881,-0.013119,-0.035007,0.077659,-0.164954,0.008560,0.045578]]
A1 = np.array(A1)
A2 = np.array(A2)

train_n = 1
test_n = 1
feature_number = 192

train_data = np.random.rand(train_n, feature_number)
test_data = np.random.rand(test_n, feature_number)
print(train_data.shape)
print(test_data.shape)
print(train_data)
print(test_data)

train_data = normalize(train_data, norm='l2')
test_data = normalize(test_data, norm='l2')



mul = 0.0
mul1 = 0.0
for i in range(0,192):
    a = A1[0][i] - A2[0][i]
    mul1 += a*a
    mul += A1[0][i] * A2[0][i]

print("mul:") 
print(2-mul*2)
print((mul+1)/2)
print(mul)
print(mul1)


pyflann.set_distance_type("euclidean")

flann = pyflann.FLANN()

branching = 10
#params = flann.build_index(train_data, algorithm='kmeans',target_precision=0.9, branching = branching , log_level='info')

#params = flann.build_index(train_data, algorithm='kdtree', trees=4)
params = flann.build_index(A1, algorithm='kmeans',target_precision=0.9, branching = branching , log_level='info')
#params = flann.build_index((A1), algorithm='kdtree', trees=4)

top_k_results = 1
#sims, dists = flann.nn_index(test_data, top_k_results, checks = params['checks'])
sims, dists = flann.nn_index((A2), top_k_results, checks = params['checks'])
print(sims.shape)
print(dists.shape)
print(sims)
print(dists)

#cosin = dists
alpha = 2.5544236818
beta  = -3.9668512074
hezi  = 1 / (1 + math.exp(alpha*(dists) + beta))

cosin = 1-dists/2
cosin = (cosin+1)/2
cosin1 = (1-dists/4)

hezi  = 1 / (1 + math.exp(alpha*(-2 * cosin + 2) + beta))



exit(0)
pickle.dump(params,open('params.pk','wb'))
#flann.save_index(b'flann_index')
# Or 
# flann_filename = 'flann_index'
# flann.save_index(bytes(flann_filename, encoding='utf8'))
